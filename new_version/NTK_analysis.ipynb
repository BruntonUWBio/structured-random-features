{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a fully trained network in the NTK regime. We have a 1-hidden layer ReLU network.\n",
    "\n",
    "$$\n",
    "f_{W, a} (x)  = \\dfrac{1}{\\sqrt{m}} \\sum_{i=1}^m a_r \\sigma(w_r^\\intercal x)\n",
    "$$\n",
    "\n",
    "The parameters are initialized randomly:\n",
    "$$\n",
    "w_r(0) \\sim \\mathcal{N}(0, \\kappa^2 \\mathbb{I}), a_r \\sim \\text{unif}(\\{-1, 1 \\}), \\forall r \\in [m]\n",
    "$$\n",
    "\n",
    "We fix the second layer $\\mathbf{a}$ and optimize the first layer $\\bf{W}$ on the quadratic objective function.\n",
    "$$\n",
    "\\phi(\\mathbf{W}) = \\dfrac{1}{2} \\sum_{i=1}^n (y_i - f_{\\mathbf{W, a (x_i)}})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from estimator import classical_weights, V1_inspired_weights_same_center\n",
    "from data_fns import load_mnist, load_cifar_10\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = V1_inspired_weights_same_center(20, 1024, 5, 2, [14, 14])\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    ax = fig.add_subplot(2, 3, i + 1)\n",
    "    plt.imshow(W[i].reshape(32, 32), cmap='RdBu')\n",
    "    plt.axis('off')\n",
    "    cmap_lim = np.max(np.abs(W[i]))\n",
    "    plt.clim(-cmap_lim, cmap_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "torch.cuda.set_device(0)\n",
    "train, train_labels, test, test_labels = load_mnist('./data/mnist/')\n",
    "# train, train_labels, test, test_labels = load_cifar_10(grayscale=True)\n",
    "\n",
    "# convert data to tensors\n",
    "train = torch.from_numpy(train).float().to('cuda')\n",
    "train_labels = torch.from_numpy(train_labels).long().to('cuda')\n",
    "test = torch.from_numpy(test).float().to('cuda')\n",
    "test_labels = torch.from_numpy(test_labels).long().to('cuda')\n",
    "\n",
    "# reshape\n",
    "train_labels = train_labels.reshape(-1, 1)\n",
    "test_labels = test_labels.reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train, test, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat similar experimental conditions as Du et. al. 2019. \n",
    "\"Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs so that |x|_2 = 1\n",
    "train /= torch.norm(train, dim=1).reshape(-1, 1)\n",
    "test /= torch.norm(test, dim=1).reshape(-1, 1)\n",
    "\n",
    "# load only the first two classes\n",
    "train_idx = torch.logical_or(train_labels == 0, train_labels == 1).flatten()\n",
    "test_idx = torch.logical_or(test_labels == 0, test_labels == 1).flatten()\n",
    "train_binary = train[train_idx]\n",
    "\n",
    "# We want label = 1 if first class and -1 otherwise.\n",
    "# So we multiply original labels by -2 and add 1. Then, 0 -> 1, 1 -> -1\n",
    "train_labels_binary = train_labels[train_idx] * -2 + 1\n",
    "test_labels_binary = test_labels[test_idx] * -2 + 1\n",
    "\n",
    "# We have 10k training samples and ~2k validation samples\n",
    "X_test, y_test = test[test_idx], test_labels_binary\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_binary, train_labels_binary, train_size=9999, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the neural network. Hidden weights are trained but the output weights are fixed.\n",
    "class NTK_net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, kappa):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        torch.nn.init.normal_(self.fc1.weight, mean=0, std=kappa ** 2)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        torch.nn.init.uniform_(self.output.weight, a=-1, b=1)\n",
    "        self.output.weight.requires_grad = False # freeze layer\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = torch.relu(self.fc1(inputs))\n",
    "        return 1 / np.sqrt(self.hidden_size) * self.output(x)\n",
    "\n",
    "class bio_NTK_net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, kappa):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc1.weight.data = torch.FloatTensor(V1_inspired_weights_same_center(hidden_size, input_size, t=5, l=2, m=[14, 14], \n",
    "                                                                                 scale=kappa ** 2))\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        torch.nn.init.uniform_(self.output.weight, a=-1, b=1)\n",
    "        self.output.weight.requires_grad = False # freeze layer\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = torch.relu(self.fc1(inputs))\n",
    "        return 1 / np.sqrt(self.hidden_size) * self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size, hidden_size, output_size, kappa = X_train.shape[1], 5000, 1, 1e-2\n",
    "network = NTK_net(input_size, hidden_size, output_size, kappa).to('cuda')\n",
    "bio_network = bio_NTK_net(input_size, hidden_size, output_size, kappa).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs, lr = 50000, 1e-3\n",
    "loss_func = nn.MSELoss(reduction='sum')\n",
    "loss_list = []\n",
    "for i in range(n_epochs):\n",
    "    optim = torch.optim.SGD(network.parameters(), lr=lr)\n",
    "    optim.zero_grad()\n",
    "    loss = loss_func(network(X_train), y_train.float()) / 2\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    loss_list.append(loss)\n",
    "    if (i % 5000 == 0):\n",
    "        print('Epoch: %d, loss=%0.3f' % (i, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs, lr = 50000, 1e-3 \n",
    "loss_func = nn.MSELoss(reduction='sum')\n",
    "loss_list_bio = []\n",
    "for i in range(n_epochs):\n",
    "    optim = torch.optim.SGD(bio_network.parameters(), lr=lr)\n",
    "    optim.zero_grad()\n",
    "    loss = loss_func(bio_network(X_train), y_train.float()) / 2\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    loss_list_bio.append(loss)\n",
    "    if (i % 10000 == 0):\n",
    "        print('Epoch: %d, loss=%0.3f' % (i, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntk_test_loss = loss_func(network(X_test), y_test)\n",
    "bio_test_loss = loss_func(bio_network(X_test), y_test)\n",
    "colors = {'NTK_bio': '#2c7fb8', 'NTK': '#d95f02'}\n",
    "\n",
    "fig = plt.figure(figsize=(10.6, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "t = np.arange(1, n_epochs + 1)\n",
    "plt.suptitle('Training loss on cifar with 2 classes; test loss NTK=%0.2f, bio=%0.2f' % (ntk_test_loss, bio_test_loss))\n",
    "plt.loglog(t, loss_list, c=colors['NTK'], label='NTK', lw=7)\n",
    "plt.loglog(t, loss_list_bio,  c=colors['NTK_bio'], label='bio NTK', lw=7)\n",
    "plt.xlabel('Epoch', fontsize=40)\n",
    "plt.ylabel('Train loss', fontsize=40)\n",
    "plt.yticks([10, 100, 1000, 10000])\n",
    "plt.xticks([1, 10, 100, 1000, 10000, 10000])\n",
    "# plt.ylim([-0.05, 1.00])\n",
    "plt.xlim([1, 50000])\n",
    "ax.tick_params(axis = 'both', which = 'major', labelsize = 30, width=2, length=6)\n",
    "# plt.legend()\n",
    "plt.savefig('results/NTK_analysis/cifar10_train_loss_h=%d_fixed_center.png' % hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kernel matrix analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "from estimator import V1_inspired_kernel_matrix\n",
    "from data_fns import load_mnist, load_cifar_10\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'mnist'\n",
    "if dataset == 'mnist':\n",
    "    train, train_labels, test, test_labels = load_mnist('./data/mnist/')\n",
    "elif dataset == 'cifar10':\n",
    "    train, train_labels, test, test_labels = load_cifar_10(grayscale=True)\n",
    "\n",
    "# normalize inputs so that |x|_2 = 1\n",
    "train /= la.norm(train, axis=1).reshape(-1, 1)\n",
    "test /= la.norm(test, axis=1).reshape(-1, 1)\n",
    "\n",
    "# load only the first two classes\n",
    "train_idx = np.logical_or(train_labels == 0, train_labels == 1).flatten()\n",
    "test_idx = np.logical_or(test_labels == 0, test_labels == 1).flatten()\n",
    "train_binary = train[train_idx]\n",
    "\n",
    "# We want label = 1 if first class and -1 otherwise.\n",
    "# So we multiply original labels by -2 and add 1. Then, 0 -> 1, 1 -> -1\n",
    "train_labels_binary = train_labels[train_idx] * -2 + 1\n",
    "test_labels_binary = test_labels[test_idx] * -2 + 1\n",
    "\n",
    "# We have 10k training samples and ~2k validation samples\n",
    "X_test, y_test = test[test_idx], test_labels_binary\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_binary, train_labels_binary, train_size=5000, random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the projection of eigenvalues of NTK kernel matrix on the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that for the network presented above, the NTK matrix is \n",
    "\n",
    "$$\n",
    "H^\\infty_{ij} = \\dfrac{x_i^\\intercal x_j (\\pi - \\arccos(x_i^\\intercal x_j))}{2 \\pi}, \\; \\; \\forall i, j \\in [n]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bio-inspired kernel matrix\n",
    "input_size, t, l, m = X_train.shape[1], 5, 2, [14, 14]\n",
    "C = V1_inspired_kernel_matrix(input_size, t, l, m, scale=1)\n",
    "C_chol = la.cholesky(C)\n",
    "X_train_proj = X_train @ C_chol\n",
    "X_train_proj /= la.norm(X_train_proj, axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regualar NTK\n",
    "H_inf = (X_train @ X_train.T) * (np.pi - np.arccos(X_train @ X_train.T)) / (2 * np.pi)\n",
    "H_inf[np.isnan(H_inf)] = 1 / 2\n",
    "\n",
    "H_inf_bio = (X_train @ X_train.T) * (np.pi - np.arccos(X_train_proj @ X_train_proj.T)) / (2 * np.pi) \n",
    "H_inf_bio[np.isnan(H_inf_bio)] = 1 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find eigenvalues, eigenvectors of the kernel matrices\n",
    "eig_val, eig_vec = la.eigh(H_inf)\n",
    "eig_val_bio, eig_vec_bio = la.eigh(H_inf_bio)\n",
    "\n",
    "# flip to arrange in descending order\n",
    "eig_val, eig_vec = np.flip(eig_val), np.flip(eig_vec, 1)\n",
    "eig_val_bio, eig_vec_bio = np.flip(eig_val_bio), np.flip(eig_vec_bio, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot eigenvalues\n",
    "i = 20\n",
    "fig = plt.figure()\n",
    "plt.title('Magnitude of eigenvalues: %s' % dataset, fontsize=16)\n",
    "plt.plot(eig_val[:i], 'k-o', label='NTK')\n",
    "plt.plot(eig_val_bio[:i],'r-o', label='NTK bio', alpha=0.6)\n",
    "plt.xlabel('Eigenvector #')\n",
    "plt.ylabel('Magnitude')\n",
    "# plt.xticks(np.arange(11))\n",
    "plt.legend()\n",
    "# plt.savefig('results/NTK_analysis/eigenvalues_%s.png' % dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the projections of eigenvectors onto the data labels\n",
    "proj = eig_vec.T @ y_train\n",
    "proj_bio = eig_vec_bio.T @ y_train\n",
    "\n",
    "i = 100\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.plot(np.abs(proj)[:i], 'k-o', label='NTK', alpha=0.6)\n",
    "plt.plot(np.abs(proj_bio)[:i], 'r-o', label='bio NTK')\n",
    "plt.legend()\n",
    "# plt.savefig('results/NTK_analysis/proj_eigenvecs_%s.png' % dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the norm of the projections \n",
    "i = 5000\n",
    "print('Norm for regular NTK', la.norm(np.abs(proj)[:i])) \n",
    "print('Norm for bio NTK', la.norm(np.abs(proj_bio)[:i]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_norm_NTK = np.cumsum(proj ** 2)\n",
    "cum_norm_NTK_bio = np.cumsum(proj_bio ** 2)\n",
    "\n",
    "i = 50\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.title('Cumulative norm of projections of y on top eigenvecs (ascending order)')\n",
    "plt.plot(cum_norm_NTK[:i], 'k-', label='NTK', alpha=0.6)\n",
    "plt.plot(cum_norm_NTK_bio[:i], 'r-', label='bio NTK')\n",
    "plt.ylabel('cumulated projection norm', fontsize=16)\n",
    "plt.xlabel('eigvector', fontsize=16)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 100\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.suptitle('Difference between cumulative proj. of NTK bio and NTK on %s' % dataset)\n",
    "plt.plot(np.array(cum_norm_NTK_bio) - np.array(cum_norm_NTK), '-', label='NTK_bio - NTK', lw=2)\n",
    "plt.plot(np.arange(5000), np.zeros(5000), '--r', label=r'$y = 0$')\n",
    "plt.ylabel('difference in cumulative projection norm')\n",
    "plt.xlabel('eigenvector')\n",
    "ax.tick_params(axis = 'both', which = 'major', labelsize = 20, width=2, length=6)\n",
    "plt.xticks(np.arange(0, 1200, 20))\n",
    "plt.yticks(np.arange(-20, 110, 20))\n",
    "plt.xlim([0, i])\n",
    "# plt.legend()\n",
    "plt.savefig('results/NTK_analysis/diff_cum_proj_eigenvecs_%s.png' % dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the projection times the eigenvalue square\n",
    "proj_eigval = proj ** 2 * eig_val\n",
    "proj_eigval_bio = proj_bio ** 2 * eig_val_bio\n",
    "\n",
    "i = 5000\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.plot(np.cumsum(proj_eigval)[:i], 'k-', label='NTK', alpha=0.6)\n",
    "plt.plot(np.cumsum(proj_eigval_bio)[:i], 'r-', label='bio NTK')\n",
    "plt.ylabel('cumulated projection norm')\n",
    "plt.xlabel('eigval #')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cumulative sum of smallest eigenvalues\n",
    "i = 5000\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.semilogy(eig_val[::-1], 'k-', label='NTK', alpha=0.6)\n",
    "plt.semilogy(eig_val_bio[::-1], 'r-', label='bio NTK')\n",
    "plt.ylabel('cumulated sum of eigenvalues')\n",
    "plt.xlabel('eigval #')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the cumulative norm of the projections to eigenvecs in the ascending order\n",
    "cum_norm_NTK_asc = np.cumsum(np.flip(proj) ** 2)\n",
    "cum_norm_NTK_bio_asc = np.cumsum(np.flip(proj_bio) ** 2)\n",
    "\n",
    "i = 5000\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.title('Difference between cumulative proj. of NTK bio and NTK on %s' % dataset)\n",
    "plt.plot(np.array(cum_norm_NTK_bio_asc)[:i] - np.array(cum_norm_NTK_asc)[:i], '-o', label='NTK_bio - NTK')\n",
    "plt.plot(np.arange(5000)[:i], np.zeros(5000)[:i], '--r', label=r'$y = 0$')\n",
    "plt.ylabel('difference in cumulative projection norm')\n",
    "plt.xlabel('eigenvector')\n",
    "# plt.xticks(np.arange(i))\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization bound for Gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(\\theta) = \\sqrt{\\frac{2y^TH^{-1}(\\theta)y}{n}}$$ \n",
    "where\n",
    "$$H(\\theta)_{ij} = \\frac{x_i^TC_\\theta x_j\\left(\\pi - \\arccos x_i^TC_\\theta x_j\\right)}{2\\pi}$$\n",
    "Let $x_i \\in [0, 1]$ then the covariance matrix $C(\\theta)$ assigns covariance coefficients to the coordinates of x that are exponentially decreasing with the distance. In other words, it makes an assumtion that nearly-placed coordinates of x are correltated:\n",
    "$$C(\\theta)_{ij} = \\exp\\left(-\\frac{(i-j)^2}{2\\theta^2n^2}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "from data_fns import load_mnist, load_cifar_10\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_matrix(n, theta, scale=1):\n",
    "    P = np.array([np.arange(n).tolist()] * n)\n",
    "    C = np.exp(-(P - P.T)**2/(2 * theta ** 2 * n ** 2))\n",
    "#     C *= (scale * n / np.trace(C))\n",
    "#     C += 1e-7 * np.eye(n)\n",
    "    return C\n",
    "\n",
    "# def cov_matrix(n, theta, scale=1):\n",
    "#     x = np.arange(np.sqrt(n))\n",
    "#     yy, xx = np.meshgrid(x, x)\n",
    "#     grid = np.column_stack((xx.flatten(), yy.flatten()))\n",
    "\n",
    "#     dist = squareform(pdist(grid, 'sqeuclidean'))\n",
    "#     C = np.exp(-dist / (2 * theta ** 2))\n",
    "    \n",
    "#     C += 1e-5 * np.eye(n)\n",
    "#     C *= (scale * n / np.trace(C))\n",
    "#     return C\n",
    "\n",
    "def H_inf(X, theta, scale=1):\n",
    "    n = X.shape[1]\n",
    "    C = cov_matrix(n, theta, scale)\n",
    "    L = la.cholesky(C)    \n",
    "    Lx = X @ L\n",
    "    Lx /= la.norm(Lx, axis=1).reshape(-1, 1)\n",
    "    \n",
    "    H = (X @ X.T) * (np.pi - np.arccos(Lx @ Lx.T)) / (2 * np.pi)\n",
    "    H[np.isnan(H)] = 1 / 2 # replace NaN with 1/2\n",
    "    return H\n",
    "\n",
    "def gen_err_measure(X, y, theta, scale=1):\n",
    "    n = X.shape[1]\n",
    "    return y.T.dot(np.linalg.inv(H_inf(X, theta, scale))).dot(y) / n\n",
    "\n",
    "def conv_measure(X, y, theta, scale=1):\n",
    "    n = X.shape[1]\n",
    "    return -y.T.dot(H_inf(X, theta, scale).dot(y))/ n\n",
    "\n",
    "def joint_L(X, y, theta, lam=1e-6, scale=1):\n",
    "    f = gen_err_measure(X, y, theta, scale=1)\n",
    "    g = conv_measure(X, y, theta, scale=1)\n",
    "    return (1-lam) * f + lam * g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100 # number of objects\n",
    "n = 10  # number of features\n",
    "\n",
    "# generate data\n",
    "theta_true = 0.5\n",
    "C_true = cov_matrix(n, theta_true, 1)\n",
    "np.random.seed(42)\n",
    "X = np.random.multivariate_normal(np.zeros(n), C_true, size=m) \n",
    "X = X / X.sum(axis=1, keepdims=True)\n",
    "beta = np.random.rand(n)\n",
    "y = X.dot(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "C = cov_matrix(n, 40)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(C, cmap='RdBu')\n",
    "plt.colorbar()\n",
    "\n",
    "np.array([np.arange(n).tolist()] * n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_span = np.linspace(40, 1500, 10)\n",
    "f_span, g_span, L_span = [], [], []\n",
    "\n",
    "for i, theta in enumerate(theta_span):\n",
    "    print('%d/%d' % (i, len(theta_span)))\n",
    "    f_span.append(gen_err_measure(X, y, theta))\n",
    "    g_span.append(conv_measure(X, y, theta))\n",
    "    L_span.append(joint_L(X, y, theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax3) = plt.subplots(nrows=1, ncols=2, figsize=(24, 10))\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel(r'$l$')\n",
    "ax1.set_ylabel('generalization bound', color=color)\n",
    "ax1.plot(theta_span, f_span, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('initial slope', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(theta_span, g_span, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax3.plot(theta_span, L_span, color=\"black\")\n",
    "ax3.set_xlabel(r'$l$')\n",
    "ax3.set_ylabel('$J_\\lambda(l)$, weighted criterion')\n",
    "ax3.set_title(r\"$J_\\lambda(l)$ with $\\lambda=1e5$\")\n",
    "\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

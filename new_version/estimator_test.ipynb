{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597763885780",
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests and implementation for the RFClassifier class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg as la\n",
    "from estimator import RFClassifier, classical_weights, V1_inspired_weights, haltere_inspired_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation for V1 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "data, labels = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)\n",
    "\n",
    "b = np.mean(la.norm(X_train, axis=1) ** 2 / X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select the linear classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logit = LogisticRegression(solver='saga')\n",
    "\n",
    "# non-linearity\n",
    "relu = lambda x: np.maximum(0, x)\n",
    "\n",
    "# classify using classical weights\n",
    "params_classical = {'weight_fun': classical_weights, 'bias': b, 'nonlinearity': relu, 'clf': logit}\n",
    "clf = RFClassifier(width=40, **params_classical)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "# classify using V1 weights\n",
    "kwargs = {'t': 5, 'l': 3}\n",
    "params_neural = {'weight_fun': V1_inspired_weights, 'kwargs': kwargs, 'bias': b, 'nonlinearity':relu, 'clf': logit}\n",
    "clf = RFClassifier(width=40, **params_neural)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation for haltere weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "data, labels = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)\n",
    "b = np.mean(la.norm(X_train, axis=1) ** 2 / X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# linear classifier\n",
    "logit = LogisticRegression(solver='saga')\n",
    "\n",
    "# non-linearity\n",
    "cos = lambda x: np.cos(x)\n",
    "relu = lambda x: np.maximum(x, 0)\n",
    "\n",
    "# params\n",
    "params_classical = {'weight_fun': classical_weights, 'bias': b, 'nonlinearity': relu, 'clf': logit}\n",
    "clf = RFClassifier(60, **params_classical)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "W_classical = clf.W_[0]\n",
    "\n",
    "kwargs = {'lowcut': 4, 'highcut': 6}\n",
    "params_neural = {'weight_fun': haltere_inspired_weights, 'kwargs': kwargs, 'bias': b, 'nonlinearity': relu, 'clf': logit}\n",
    "clf = RFClassifier(60, **params_neural)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "W_neural = clf.W_[0]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(W_classical, c='k')\n",
    "plt.plot(W_neural)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time the linearSVM vs logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from estimator import RFClassifier, classical_weights, relu, haltere_inspired_weights, V1_inspired_weights\n",
    "from data_fns import frequency_XOR, load_mnist\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = frequency_XOR(60000, 1000, 5, 8, 0.784, 0.99, random_state=5, shuffle=True)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=5)\n",
    "# b = np.mean(la.norm(X_train, axis=1) ** 2 / X_train.shape[1])\n",
    "X_train, y_train, X_test, y_test = load_mnist('data/mnist/')\n",
    "b = np.mean(la.norm(X_train, axis=1) ** 2 / X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.785\n3.062 s\n"
    }
   ],
   "source": [
    "# start = time.time()\n",
    "# # logistic regression\n",
    "# logit = LogisticRegression(dual=False, solver='saga', n_jobs=5)\n",
    "# params = {'width': 20, 'weight_fun': classical_weights, 'bias': b, 'nonlinearity': relu, 'clf': logit}\n",
    "# clf = RFClassifier(**params)\n",
    "# clf.fit(X_train, y_train)\n",
    "# print(clf.score(X_test, y_test))\n",
    "# end = time.time()\n",
    "# print('%0.3f s' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Iter 0: score=0.978, 121.153 s\nIter 1: score=0.975, 102.257 s\n111.7051830291748 9.447813034057617\n"
    }
   ],
   "source": [
    "time_taken = []\n",
    "for i in range(2):\n",
    "    start = time.time()\n",
    "    svm = LinearSVC(dual=True, max_iter=200)\n",
    "    kwargs = {'t': 5, 'l':3}\n",
    "    params = {'width': 1000, 'weight_fun': V1_inspired_weights, 'kwargs': kwargs, 'bias': b, 'nonlinearity': relu, 'clf': svm}\n",
    "    clf = RFClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    end = time.time()\n",
    "    print('Iter %d: score=%0.3f, %0.3f s' % (i, score, end - start))\n",
    "    time_taken.append(end - start)\n",
    "print(np.mean(time_taken), np.std(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Iter 0: score=0.980, 150.549 s\nIter 1: score=0.978, 158.188 s\n154.36838352680206 3.819311261177063\n"
    }
   ],
   "source": [
    "time_taken = []\n",
    "for i in range(2):\n",
    "    start = time.time()\n",
    "    svm = LinearSVC(dual=False, max_iter=200)\n",
    "    kwargs = {'t': 5, 'l':3}\n",
    "    params = {'width': 1000, 'weight_fun': V1_inspired_weights, 'kwargs': kwargs, 'bias': b, 'nonlinearity': relu, 'clf': svm}\n",
    "    clf = RFClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    end = time.time()\n",
    "    print('Iter %d: score=%0.3f, %0.3f s' % (i, score, end - start))\n",
    "    time_taken.append(end - start)\n",
    "print(np.mean(time_taken), np.std(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Iter 0: score=0.049, 100.192 s\nIter 1: score=0.041, 118.715 s\n109.45346260070801 9.261189460754395\n"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "time_taken = []\n",
    "for i in range(2):\n",
    "    start = time.time()\n",
    "    sgd = SGDClassifier(loss='squared_hinge', alpha=1, shuffle=True, n_jobs=5, learning_rate='optimal', max_iter=500)\n",
    "    kwargs = {'t': 5, 'l':3}\n",
    "    params = {'width': 1000, 'weight_fun': V1_inspired_weights, 'kwargs': kwargs, 'bias': b, \n",
    "'nonlinearity': relu, 'clf': sgd}\n",
    "    clf = RFClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    score = clf.score(X_test, y_test)\n",
    "    end = time.time()\n",
    "    print('Iter %d: score=%0.3f, %0.3f s' % (i, 1 - score, end - start))\n",
    "    time_taken.append(end - start)\n",
    "print(np.mean(time_taken), np.std(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "316"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}